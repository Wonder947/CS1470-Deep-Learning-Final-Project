{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "340ca66f",
   "metadata": {},
   "source": [
    "# Modern Hopfield Networks on the CIFAR10 dataset\n",
    "**2025 Spring CSCI1470 Deep Learning Final Project**\n",
    "\n",
    "**Authors: [Haosheng Wang](https://github.com/Wonder947), Edrick Guerrero, [Alfonso Gordon Cabello de los Cobos](https://github.com/AlfonsoR-GordonCC)**\n",
    "\n",
    "In this project we have created a Hopfiel Network that memorizes patterns on a grayscale version of the CIFAR10 dataset for it to be able to reconstruct the images from altered samples using TensorFlow.\n",
    "\n",
    "We followed one of the pytorch implementation https://github.com/ml-jku/hopfield-layers, proposed by the paper *Hopefield Network is All You Need*.\n",
    "\n",
    "The goals for this project are:\n",
    "1. BASE GOAL: high accuracy with small part of the dataset\n",
    "2. TARGET GOAL: analyze how accuracy is affected by the quality (i.e. correlation between\n",
    "images) and quantity (number of images to store) of the stored images/features.\n",
    "3. STRETCH GOAL: analyze how accuracy is affected by the quality (e.g. amount of noise,\n",
    "completeness of features) of the images/features used for retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b28d81",
   "metadata": {},
   "source": [
    "# Division of Labor\n",
    "- Haosheng Wang:\n",
    "- Edrick Guerrero:\n",
    "- Alfonso Gordon Cabello de los Cobos:\n",
    "\n",
    "For BASE GOAL:\n",
    "1. ✅ Test existing pytorch implementation, including\n",
    "    - Testing the performance of existing implementation of modern hopfield network by the paper author as proof of concept\n",
    "    - This shall walk through the whole process(1. load several images to be stored 2. add simple mask/noise to those images 3. init modern hopfield network 4. store the images 5. get retreived images using masked images 6. visualize the original image, the masked image, and the retrieved image), and produce a demo for later steps to follow\n",
    "    - NOTE: this is trivially done, as we later found existing notebook already done the job https://github.com/ml-jku/hopfield-layers/blob/gh-pages/examples/simpsons/continuous_hopfield_pytorch.ipynb\n",
    "\n",
    "2. ✅ Load dataset and image processing, including\n",
    "    - Writing methods for loading dataset and taking part of the image dataset for memorizing\n",
    "    - Writing methods for converting Images to gray-scale\n",
    "    - Writing methods for converting between Image (for visualization e.g. PIL Image) and Numpy Array (or Tensorflow tensor) back and forth\n",
    "    - Writing methods for corrupting the image (e.g. adding mask; adding noise)\n",
    "3. Modern hopfield netork core implementation, including\n",
    "    - Implement the class of modern hopfield network, using the same or similar architecture proposed by the paper\n",
    "    - Including at least init, store, retrieve\n",
    "4. Evaluation, including\n",
    "    - Define how to measure the \"accuracy\" between original Image and retrieved Image\n",
    "    - Use the above mentioned methods to write evaluation functions for the whole process, given input images to be stored, choice of masking quality(or images for retrieval), output hopfield network performance\n",
    "    - Run the function to compare the performance of the pytorch one and our tensorflow one\n",
    "\n",
    "For TARGET GOAL:\n",
    "1. Analyze how accuracy is affected by the quality and quantity of images to be stored, including:\n",
    "    - Define how to measure the quality of images to be memorized/stored, e.g. define how to measure correlation between images\n",
    "    - Write function for the measurement, given input a batch of images to be measured, output the quality\n",
    "    - Visualize the relationship between the quality of the stored images and model accuracy (under same image quantity)\n",
    "    - Visualize the relationship between the quantity of the stored images and model accuracy (under the same image quality)\n",
    "\n",
    "For STRETCH GOAL:\n",
    "1. Analyze how accuracy is affected by the quality of the masked images for retrieval, including:\n",
    "    - Define how to measure the quality of masked images, e.g. percentage of pixels masked; norm of noises added\n",
    "    - Write function for the measurement, given input a pair of original and masked image, output quality of the masked image e.g. quality a number from 0-1 where quality 1 means the masked image is same as original image, 0 means \"completely different\"\n",
    "    - Visualize the relationship between the quality of the masked/cropped images for retrieval and final accuracy (under same store Image quality and quantity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eb1701",
   "metadata": {},
   "source": [
    "# Image preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493f65e",
   "metadata": {},
   "source": [
    "As with any deep learning library, we have to preprocess the data we want to preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeff931",
   "metadata": {},
   "source": [
    "First we import the needed libraries for the project to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16be3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib as mat\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14745d9b",
   "metadata": {},
   "source": [
    "Then we import the CIFAR10 dataset and isolate samples according to the goals of the project. Those samples must be in grayscale ffor the Network to memorize patterns correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0116ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "assert x_train.shape == (50000, 32, 32, 3)\n",
    "assert x_test.shape == (10000, 32, 32, 3)\n",
    "assert y_train.shape == (50000, 1)\n",
    "assert y_test.shape == (10000, 1)\n",
    "\n",
    "complete_imgs = np.concatenate((x_train, x_test), axis=0) # Create a Numpy Array with all the images\n",
    "\n",
    "percentage = input(\"What percentage of the CIFAR10 dataset (60000 images) do you want to use?\")\n",
    "if percentage == '': #If the user has not decided a percentage, 10% is default\n",
    "    percentage = 10\n",
    "else:\n",
    "    percentage = int(round(float(percentage)))\n",
    "\n",
    "number_images_used = int(round(60000 * (percentage/100))) # Number of images that will be used\n",
    "\n",
    "print(f'Taking a {percentage}% of the CIFAR10 dataset. Total of {number_images_used} images used.\\n')\n",
    "\n",
    "images_used = np.zeros(shape=(number_images_used, complete_imgs.shape[1], complete_imgs.shape[2], 1)) # Initialize a zero array with the same size of the complete\n",
    "\n",
    "used_indices = set()\n",
    "for i in range(number_images_used): # Lets use random images of the dataset\n",
    "    n = np.random.randint(0, complete_imgs.shape[0])\n",
    "    if n not in used_indices:\n",
    "        images_used[i] = tf.image.rgb_to_grayscale(complete_imgs[n]) # Changes the images to grayscale\n",
    "        used_indices.add(n)\n",
    "\n",
    "\n",
    "print(\"Images selected.\\n Checking if they are in grayscale...\")\n",
    "plt.imshow(images_used[0], cmap='gray')\n",
    "print(\"Cool, we can continue. Here you have one of the images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4793dc2",
   "metadata": {},
   "source": [
    "After getting the images and turned them into grayscale, is time to create the testing set by adding noise to the oringinal images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ac340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add_gaussian_noise(images, mean=0.0, std=5):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to a batch of grayscale images.\n",
    "\n",
    "    Parameters:\n",
    "        images (numpy.ndarray): Array of shape (N, H, W) or (H, W) for a single image\n",
    "        mean (float): Mean of the Gaussian noise\n",
    "        std (float): Standard deviation of the Gaussian noise\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Noisy images clipped between 0 and 1 (or 0 and 255 if your images are uint8)\n",
    "    \"\"\"\n",
    "    if images.dtype == np.uint8:\n",
    "        images = images / 255.0  # Normalize if needed\n",
    "\n",
    "    noise = np.random.normal(mean, std, images.shape)\n",
    "    noisy_images = images + noise\n",
    "    return noisy_images\n",
    "\n",
    "noisy_imgs = add_gaussian_noise(images_used)\n",
    "\n",
    "plt.figure\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(images_used[0], cmap='grey')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(noisy_imgs[0], cmap='grey')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ac340",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_imgs = np.zeros(shape=(number_images_used, complete_imgs.shape[1], complete_imgs.shape[2], 3))\n",
    "actual_index = 0\n",
    "for i in used_indices: \n",
    "    original_img = complete_imgs[i] # We get the original image for applying noise\n",
    "\n",
    "    \n",
    "    original_img_float = tf.cast(original_img, tf.float32) # Convert the image to float32 explicitly\n",
    "\n",
    "    # Generate stronger noise\n",
    "    stddev = 0.0001  # Higher value for more visible noise\n",
    "    noise = 0.1 * tf.random.normal(shape=original_img.shape, mean=0.0, stddev=stddev, dtype=tf.float32)\n",
    "\n",
    "    \n",
    "    noisy_img = original_img_float + noise # Add noise - using simple addition instead of tf.add to avoid type issues\n",
    "\n",
    "    \n",
    "    noisy_imgs[actual_index] =  tf.math.floormod(noisy_img, 1.0) # Apply module to keep in range [0,1] and save it on the array as grayscale\n",
    "    \n",
    "\n",
    "    if actual_index == 0:\n",
    "        print(\"Checking if noise is added correctly...\")\n",
    "\n",
    "        print(f\"Noisy image shape: {noisy_img.shape}, dtype: {noisy_img.dtype}\")\n",
    "\n",
    "        \n",
    "        plt.figure(figsize=(12, 4)) # Visualize with an explicit check to make sure images are compatible with imshow\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(\"Original\")\n",
    "        plt.imshow(np.array(original_img)) # Convert to numpy if needed\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(\"Noise\")\n",
    "        \n",
    "        visual_noise = (noise - tf.reduce_min(noise)) / (tf.reduce_max(noise) - tf.reduce_min(noise)) # Normalize noise for visualization\n",
    "        plt.imshow(np.array(visual_noise))\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(\"Noisy Image\")\n",
    "        plt.imshow(np.array(noisy_imgs[actual_index]))\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Cool, we can continue. Here you have one of the images\")\n",
    "    \n",
    "    noisy_imgs[actual_index] =  tf.image.rgb_to_grayscale(noisy_imgs[actual_index]) # Turn the noisy image to grayscale\n",
    "\n",
    "    actual_index += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612c0419",
   "metadata": {},
   "source": [
    "Another option is to apply a mask to the images. This way, only half of the imageis recognizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612c0419",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_masked_np = np.copy(images_used)\n",
    "count = 0\n",
    "for img in images_masked_np:\n",
    "    img[16:, :] = (255//2)\n",
    "    img[16:, :] = 0\n",
    "    images_masked_np[count] = img\n",
    "    count += 1\n",
    "\n",
    "print(\"Checking if making is done correctly...\")\n",
    "plt.imshow(images_masked_np[0], cmap='grey')\n",
    "print(\"Cool, images are sliced, we can continue. Here you have one of the images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cb57ba",
   "metadata": {},
   "source": [
    "Now we define simple functions for changing the images to NumPy arrays and viceversa, an easy way to call the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cb57ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_arr(img, new_shape = (-1, 1)):\n",
    "    return tf.reshape(img, new_shape)\n",
    "\n",
    "def arr_to_img(arr):\n",
    "    if tf.is_tensor(arr):\n",
    "        return tf.reshape(arr, shape = [32, 32, 1])\n",
    "    else:\n",
    "        tens_arr = tf.convert_to_tensor(arr)\n",
    "        return tf.reshape(tens_arr, shape = [32, 32, 1])\n",
    "    \n",
    "print(\"Checking if functions work as they should...\")\n",
    "plt.imshow(arr_to_img(img_to_arr(images_masked_np[0])), cmap='grey') \n",
    "print(\"Cool, the functions work, we can continue. Here you have one of the images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4dbee7",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e776d8fc",
   "metadata": {},
   "source": [
    "This is where the fun starts! We are going to create the class for the Hopfield Network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca62f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modern_hopfield_tf import Hopfield_tf  # optionally, import it\n",
    "\n",
    "class Hopfield_tf(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Modern Hopfield Network (single-head) with:\n",
    "      - static vs. non-static (learned) K/Q projections\n",
    "      - iterative updates on the associative state ξ\n",
    "      - optional final output projection\n",
    "    \n",
    "    ξ₀ = softmax(β · QKᵀ)\n",
    "    ξₜ₊₁ = softmax(β · (ξₜ K) Kᵀ)\n",
    "    output = ξ_final · V\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 static=True,\n",
    "                 hid_dim=None,\n",
    "                 scaling=1.0,\n",
    "                 update_steps_max=1,\n",
    "                 update_steps_eps=1e-4,\n",
    "                 output_dim=None,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.static = static\n",
    "        if not static:\n",
    "            assert hid_dim is not None, \"must specify hid_dim in non-static mode\"\n",
    "        self.hid_dim = hid_dim\n",
    "        self.scaling = scaling\n",
    "        self.update_steps_max = int(update_steps_max)\n",
    "        self.update_steps_eps = update_steps_eps\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        ks_shape, qs_shape, vs_shape = input_shape\n",
    "        # allow K and Q to have different raw dims\n",
    "        self.pattern_dim_k = ks_shape[-1]\n",
    "        self.pattern_dim_q = qs_shape[-1]\n",
    "        assert vs_shape[-1] == self.pattern_dim_k, \"V must match K in last dim\"\n",
    "\n",
    "        if self.static:\n",
    "            assert self.pattern_dim_k==self.pattern_dim_q\n",
    "\n",
    "        if not self.static:\n",
    "            # only project once, in build:\n",
    "            self.k_proj = self.add_weight(\n",
    "                name=\"k_proj\",\n",
    "                shape=(self.pattern_dim_k, self.hid_dim),\n",
    "                initializer=\"glorot_uniform\",\n",
    "                trainable=True)\n",
    "            self.q_proj = self.add_weight(\n",
    "                name=\"q_proj\",\n",
    "                shape=(self.pattern_dim_q, self.hid_dim),\n",
    "                initializer=\"glorot_uniform\",\n",
    "                trainable=True)\n",
    "\n",
    "        if self.output_dim is not None:\n",
    "            # V always lives in raw pattern_dim_k space → project to output_dim\n",
    "            self.out_proj = self.add_weight(\n",
    "                name=\"out_proj\",\n",
    "                shape=(self.pattern_dim_k, self.output_dim),\n",
    "                initializer=\"glorot_uniform\",\n",
    "                trainable=True)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: tuple of (ks, qs, vs)\n",
    "          ks: (n_keys,  pattern_dim)\n",
    "          qs: (n_queries, pattern_dim)\n",
    "          vs: (n_keys,  pattern_dim)\n",
    "        Returns:\n",
    "          output: (n_queries, output_dim or pattern_dim)\n",
    "          attn_weights: (n_queries, n_keys)\n",
    "        \"\"\"\n",
    "        ks, qs, vs = inputs\n",
    "\n",
    "        # 1) if non-static: project into associative space\n",
    "        if self.static:\n",
    "            k = ks        # (n_keys,  pattern_dim)\n",
    "            q = qs        # (n_queries, pattern_dim)\n",
    "        else:\n",
    "            k = tf.matmul(ks, self.k_proj)  # → (n_keys,  hid_dim)\n",
    "            q = tf.matmul(qs, self.q_proj)  # → (n_queries, hid_dim)\n",
    "\n",
    "        v = vs  # values remain in original space\n",
    "\n",
    "        # 2) iterative Hopfield retrieval\n",
    "        q_current = q\n",
    "        xi = None\n",
    "\n",
    "        for _ in range(self.update_steps_max):\n",
    "            # compute scaled affinities\n",
    "            scores = tf.matmul(q_current, k, transpose_b=True)  # (n_queries, n_keys)\n",
    "            scores = scores * tf.cast(self.scaling, scores.dtype)   # apply scaling -- similar to temperature\n",
    "\n",
    "            # softmax → association matrix\n",
    "            xi_new = tf.nn.softmax(scores, axis=-1)            # (n_queries, n_keys)\n",
    "\n",
    "            # retrieve new queries: q_new = xi_new @ k\n",
    "            q_new = tf.matmul(xi_new, k)                       # (n_queries, hid_dim or pattern_dim)\n",
    "\n",
    "            # check convergence in q-space\n",
    "            diff = tf.norm(q_new - q_current, ord='euclidean')\n",
    "            q_current = q_new\n",
    "            xi = xi_new\n",
    "\n",
    "            if diff < self.update_steps_eps:\n",
    "                break\n",
    "\n",
    "        # 3) final retrieval of values\n",
    "        output = tf.matmul(xi, v)                             # (n_queries, pattern_dim)\n",
    "\n",
    "        # 4) optional output projection\n",
    "        if self.output_dim is not None:\n",
    "            output = tf.matmul(output, self.out_proj)         # (n_queries, output_dim)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78f0f22",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c7c3c6",
   "metadata": {},
   "source": [
    "Now we have to know how many variables do we want to train with and how many are we leaving for testing. Althought the choice can be up to you, we will create a default scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9390506",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.copy(images_used)\n",
    "test_set = images_masked_np\n",
    "test_set = noisy_imgs\n",
    "\n",
    "\n",
    "print(\"Checking that we assigned them correctly...\")\n",
    "plt.figure\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(train_set[0], cmap='grey')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(test_set[0], cmap='grey')\n",
    "print(\"Cool, they are different, we can continue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "f669f57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch model\n",
    "\n",
    "# test pytorch version of hopfield network\n",
    "\n",
    "import torch\n",
    "from hflayers import Hopfield\n",
    "\n",
    "hopfield_args = {\n",
    "        # do not project layer input\n",
    "        'normalize_hopfield_space':False,\n",
    "        'state_pattern_as_static':True,\n",
    "        'stored_pattern_as_static':True,\n",
    "        'pattern_projection_as_static':True,\n",
    "\n",
    "        # do not pre-process layer input\n",
    "        'normalize_stored_pattern':False,\n",
    "        'normalize_stored_pattern_affine':False,\n",
    "        'normalize_state_pattern':False,\n",
    "        'normalize_state_pattern_affine':False,\n",
    "        'normalize_pattern_projection':False,\n",
    "        'normalize_pattern_projection_affine':False,\n",
    "        \n",
    "        # do not post-process layer output\n",
    "        'disable_out_projection':True\n",
    "        }\n",
    "\n",
    "image_size = 32*32\n",
    "N = image_size\n",
    "hop_pt = Hopfield(input_size=N, scaling=1.0, **hopfield_args)\n",
    "\n",
    "# arg 1 is the train images to be mapped, arg 2 shall be the masked image used for retrieving, arg 3 is the mapping back train images (in our case shall be the same as arg1)\n",
    "# can think of arg 1,2,3 as key, query, value\n",
    "store_num = 10\n",
    "query_num = store_num\n",
    "stored = train_set[:store_num].copy().reshape((store_num, -1))\n",
    "masked = test_set[:query_num].copy().reshape((query_num, -1))\n",
    "# masked = stored # extreme case, see if retrieving with same images work\n",
    "print(train_set.shape)\n",
    "print(stored.shape)\n",
    "print(masked.shape)\n",
    "\n",
    "# the pytorch version expected batched input\n",
    "stored_pt = torch.as_tensor(stored).unsqueeze(0)\n",
    "masked_pt = torch.as_tensor(masked).unsqueeze(0)\n",
    "reconstructed_pt = hop_pt((stored_pt, masked_pt, stored_pt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "f9004d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 1024])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our model\n",
    "\n",
    "# get our static model\n",
    "hop_tf = Hopfield_tf(static=True)\n",
    "\n",
    "# convert to tensor annd desired shape\n",
    "def reshape_tensor(x):\n",
    "    res = tf.convert_to_tensor(x)\n",
    "    res = tf.reshape(res, shape=(res.shape[0], -1))\n",
    "    return res\n",
    "\n",
    "# k -- train -- (num_keys, hid_dim)\n",
    "# q -- test -- (num_qs, hid_dim)\n",
    "# v -- train -- same as k\n",
    "stored_tf = reshape_tensor(stored)\n",
    "masked_tf = reshape_tensor(masked)\n",
    "\n",
    "reconstructed_tf = hop_tf((stored_tf, masked_tf, stored_tf))\n",
    "\n",
    "reconstructed_tf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "5e19162b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x127b284efe0>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stored = [arr_to_img(t) for t in stored]\n",
    "masked = [arr_to_img(t) for t in masked]\n",
    "reconstructed_set_pt = [arr_to_img(t) for t in reconstructed_pt[0]]\n",
    "reconstructed_set_tf = [arr_to_img(t) for t in reconstructed_tf]\n",
    "\n",
    "# take a look at the original image, the masked image, and the retrieved image\n",
    "idx = 2\n",
    "plt.figure\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(stored[idx], cmap='grey')\n",
    "plt.title(\"Original image\")\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(masked[idx], cmap='grey')\n",
    "plt.title(\"Noisy image\")\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(reconstructed_set_tf[idx], cmap='grey')\n",
    "plt.title(\"Our implementation\")\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(reconstructed_set_pt[idx], cmap='grey')\n",
    "plt.title(\"Original implementation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a5afaf",
   "metadata": {},
   "source": [
    "We will now run testing on our reconstructed images and compare them against our original test. We will first use MSE to calculate the noise level of original images and noisy images. Then, we will use a CLIP model to vectorize the reconstructed images and use COSINE similarity to compare these vectors and return a similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afeedb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "def calculate_average_mse(original_images, altered_images):\n",
    "    \"\"\"\n",
    "    Calculate the average Mean Squared Error (MSE) between corresponding original and altered images.\n",
    "\n",
    "    Parameters:\n",
    "    original_images (list of numpy arrays): List of original 32x32 images\n",
    "    altered_images (list of numpy arrays): List of altered 32x32 images (same length as original_images)\n",
    "\n",
    "    Returns:\n",
    "    float: Average MSE across all image pairs\n",
    "    \"\"\"\n",
    "    if len(original_images) != len(altered_images):\n",
    "        raise ValueError(\"The number of original and altered images must be the same.\")\n",
    "\n",
    "    total_mse = 0.0\n",
    "    num_images = len(original_images)\n",
    "\n",
    "    for orig_img, alt_img in zip(original_images, altered_images):\n",
    "\n",
    "        # orig_img = np.array(orig_img)\n",
    "        # alt_img = np.array(alt_img)\n",
    "\n",
    "        if orig_img.shape != alt_img.shape:\n",
    "            raise ValueError(\"All image pairs must be the same size!\")\n",
    "\n",
    "        # Calculate MSE for this image pair\n",
    "        mse = np.mean((orig_img - alt_img) ** 2)\n",
    "        total_mse += mse\n",
    "\n",
    "    average_mse = total_mse / num_images\n",
    "    return average_mse\n",
    "\n",
    "\n",
    "def calculate_vector_similarity(original_images, altered_images):\n",
    "    \"\"\"\n",
    "    Calculate vectorized similarity between original and altered images using a pre-trained CLIP model. There\n",
    "    is an assumption that the lists are corresponding (original_images[i] corresponds to altered_images[i])\n",
    "\n",
    "    Parameters:\n",
    "    original_images (list of numpy arrays): List of original 32x32 images\n",
    "    altered_images (list of numpy arrays): List of altered 32x32 images (same length as original_images)\n",
    "\n",
    "    Returns:\n",
    "    list: accuracy for images pairs\n",
    "    \"\"\"\n",
    "\n",
    "    model = SentenceTransformer(\"clip-ViT-B-32\")\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for orig_img, alt_img in zip(original_images, altered_images):\n",
    "\n",
    "        orig_img = tf.expand_dims(orig_img, axis=1)\n",
    "        orig_img = tf.expand_dims(orig_img, axis=1)\n",
    "\n",
    "        alt_img = tf.expand_dims(alt_img, axis=1)\n",
    "        alt_img = tf.expand_dims(alt_img, axis=1)\n",
    "\n",
    "        orig_img_pil = tf.keras.preprocessing.image.array_to_img(orig_img)\n",
    "        alt_img_pil = tf.keras.preprocessing.image.array_to_img(alt_img)\n",
    "\n",
    "        joined_image_list = [orig_img_pil, alt_img_pil]\n",
    "        encoded_image = model.encode(\n",
    "            joined_image_list,\n",
    "            batch_size=2,\n",
    "            convert_to_tensor=True,\n",
    "            show_progress_bar=False,\n",
    "        )\n",
    "\n",
    "        processed_images = util.paraphrase_mining_embeddings(encoded_image)\n",
    "\n",
    "        for score, image_id1, image_id2 in processed_images:\n",
    "            scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd83c683",
   "metadata": {},
   "source": [
    "# Reference / Public Implementation Links\n",
    "1. (the original paper github repo) https://github.com/ml-jku/hopfield-layers\n",
    "2. (the blog) https://ml-jku.github.io/hopfield-layers/\n",
    "3. (the original paper) https://arxiv.org/abs/2008.02217\n",
    "4. (base_1 reference notebook) https://github.com/ml-jku/hopfield-layers/blob/gh-pages/examples/simpsons/continuous_hopfield_pytorch.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs1470_final_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
