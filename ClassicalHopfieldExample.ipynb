{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hopfield Network\n",
    "\n",
    "This is a demonstration of classical hopfield network.\n",
    "\n",
    "Following the blog: https://ml-jku.github.io/hopfield-layers/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imageio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# image processing\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimageio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01miio\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imageio'"
     ]
    }
   ],
   "source": [
    "# get images\n",
    "import os\n",
    "\n",
    "# basic data processing\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "# image processing\n",
    "from PIL import Image\n",
    "import imageio as iio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Hopfield Network\n",
    "\n",
    "Associative memories are one of the earliest artificial neural models dating back to the 1960s and 1970s. Best known are Hopfield Networks, presented by John Hopfield in 1982. As the name suggests, the main purpose of associative memory networks is to associate an input with its most similar pattern. In other words, the purpose is to store and retrieve patterns.\n",
    "\n",
    "The simplest associative memory is just a sum of outer products of the N patterns $\\{{x_i}\\}^N_{i=1}$ that we want to store (Hebbian learning rule). In classical Hopfield Networks these patterns are polar (binary), i.e. $x_i∈\\{−1,1\\}^d$, where d is the length of the patterns. The corresponding weight matrix W is:\n",
    "\n",
    "$$W=\\sum_{i}^{N}{x_ix_i^T} \\tag{1}$$\n",
    "\n",
    "The weight matrix W stores the patterns, which can be retrieved starting with a state pattern $\\xi$.\n",
    "\n",
    "From now on we denote the N stored patterns as $\\{{x_i}\\}_{i=1}^N$ and any state pattern or state as $ξ$.\n",
    "\n",
    "The basic synchronuous update rule is to repeatedly multiply the state pattern $ξ$ with the weight matrix W, subtract the bias and take the sign:\n",
    "\n",
    "$$\\xi^{t+1} = sign(W\\xi^T - b) \\tag{2}$$\n",
    "\n",
    "where $b∈R^d$ is a bias vector, which can be interpreted as threshold for every component. The asynchronous update rule performs this update only for one component of $ξ$\n",
    " and then selects the next component for update. Convergence is reached if $ξ^{t+1}=ξ^t$.\n",
    "\n",
    "The asynchronous version of the update rule of Eq. (2) minimizes the energy function E:\n",
    "\n",
    "$$E = -\\frac{1}{2} \\xi^T W \\xi + \\xi^T b = -\\frac{1}{2} \\sum_{i=1}^d \\sum_{j=1}^d {w_{ij} \\xi_i \\xi_j} + \\sum_{i=1}^d{b_i \\xi_i} \\tag{3}$$\n",
    "\n",
    "As derived in the papers of Bruck, Goles-Chacc et al. and the original Hopfield paper, the convergence properties are dependent on the structure of the weight matrix W\n",
    " and the method by which the nodes are updated:\n",
    "\n",
    "- For asynchronous updates with wii≥0 and wij=wji, the updates converge to a stable state.\n",
    "- For synchronous updates with wij=wji, the updates converge to a stable state or a limit cycle of length 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalHopfieldNetwork:\n",
    "    def __init__(self, hid_dim:int):\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "        self.W = np.zeros((hid_dim, hid_dim))\n",
    "        self.b = np.zeros((hid_dim, 1))\n",
    "\n",
    "    def store(self, store_patterns: NDArray):\n",
    "        '''\n",
    "        Stores the patterns to be memorized, save in self.W, which will be the sum of the outer products of the n patterns to be stored\n",
    "\n",
    "        Args:\n",
    "            store_patterns: a (n, hid_dim, 1) numpy array, where n is number of store patterns, each pattern is (hid_dim, 1)\n",
    "        \n",
    "        Return:\n",
    "            None\n",
    "        '''\n",
    "        assert len(store_patterns.shape) == 3   # shape: (batch_size, hid_dim, 1)\n",
    "        assert store_patterns.shape[1] == self.hid_dim\n",
    "\n",
    "        for x in store_patterns:\n",
    "            self.W += x@x.T\n",
    "\n",
    "\n",
    "\n",
    "    def retrieve(self, state_pattern: NDArray):\n",
    "        '''\n",
    "        Retrieve the given state_pattern, by update current state until converge.\n",
    "\n",
    "        Args:\n",
    "            state_pattern: a (hid_dim, 1) numpy array\n",
    "\n",
    "        Return:\n",
    "            retrieved_pattern: retrieved_pattern\n",
    "        '''\n",
    "        # assert len(state_pattern.shape) == 1\n",
    "        # assert state_pattern.shape[0] == self.hid_dim\n",
    "\n",
    "        retrieved_pattern = self.update(state_pattern)\n",
    "\n",
    "        return retrieved_pattern\n",
    "\n",
    "\n",
    "    def update(self, state):\n",
    "        '''\n",
    "        Update by repeatedly do basic synchronuous update step: new_state = sign(W@old_state-b), until convergence.\n",
    "        Convergence is reached if new energy is (almost) the same as old energy. \n",
    "\n",
    "        Args:\n",
    "            state: a (hid_dim, 1) numpy array\n",
    "        \n",
    "        Return:\n",
    "            cur_state: a (hid_dim, 1) numpy array\n",
    "        '''\n",
    "        prev_state = state\n",
    "        cur_state = self.update_step(prev_state)\n",
    "        while abs(self.get_energy(prev_state)-self.get_energy(cur_state))>1e-8:\n",
    "            prev_state = cur_state\n",
    "            cur_state = self.update_step(cur_state)  \n",
    "        \n",
    "        return cur_state\n",
    "\n",
    "\n",
    "    def update_step(self, state):\n",
    "        '''\n",
    "        Perform one update step: new_state = sign(W@old_state-b)\n",
    "        '''\n",
    "        new_state = np.sign(self.W@state-self.b)\n",
    "\n",
    "        return new_state\n",
    "\n",
    "\n",
    "    def get_energy(self, state):\n",
    "        '''\n",
    "        Get energy of state: E = -1/2 * ξ.T W ξ + ξ.T b\n",
    "\n",
    "        Return:\n",
    "            a scalar\n",
    "        '''\n",
    "        energy = -1/2 * state.T @ self.W @ state + state.T@self.b\n",
    "\n",
    "        return energy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "We start with an illustrative example of a Hopfield Network. One input image should first be stored and then be retrieved. The input image is an image.\n",
    "\n",
    "We preprocess the image by\n",
    "- resize to shape of 64x64\n",
    "- convert to black and white (mode=\"1\")\n",
    "- convert to numpy array\n",
    "- map from 0,1 to -1,1\n",
    "- flatten into 1d array as features\n",
    "\n",
    "To convert the retrieved numpy array of classical hopfield network back to image, we do\n",
    "- reshape from 1d array to img_size\n",
    "- map from -1,1 to 0,1\n",
    "- convert to Image type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(406, 407) RGBA PNG\n",
      "(4096, 1)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAACfklEQVR4AZWWAZakMAhEdd/c/8ouBSaBonB2nTdtjNSnIJru+7l+O24LmKP+/Ca/oLcPP4nge2ZD95gOEe5CxsrJk4jz9nRfJdwk72o3edK10evf570IK4UpHw7u+0nRGNpMa+YHAIkLwS6oKEBTCBT76GZxKxcVobMDSe6TA4AXYBtrTRgApfqtVoMJ0L2+aragAb3ZJzkRNCAv35HK0QCQsXJSAr4qYIoC6GeIle+1Aowr4BqypwAUQqkJLwD/VcHVAa7/NpE9ybcR8tdpkIrtKvnJNB+H2t4mU61qsobMdUAkd/WycdnmXEykrK0HZx97kmiSX6KJh55V+IKQBztI1WoJF8MAsW1GYlhQ+xQBELWc5grS9AnwQAJk0c0JN3qlwJkAtobGkJEWnPELIp6DXUaNj3WwHlY8A3D7FdbAwUAvYetrKv+loIrrPVjF2Xn/LsG36mus1iWamLI8Vo//4fH2gliNJH3uDY1m4GKHlIvXqQDQKm65zcPIuXaEANSYlNVi0ctK4GVEC0qEXcTmglJAo6OtQip5he5fOqYnvFgFkcSYMWuk4g4ZRA9W4nJGHLL/gwNpAWrIm749yj0ijLh1+2glUBMnPYotJe0LAvQMO3IYMGDI86pbAW0Z0WZm8M5WrbCDIBjDQfE2irwH0h5lIzx798CoyunSEtX7i9ynY6avEpewCdSK05gz8tgBgFbUI5z2+QGAr4eMeMd2wj6XjwngT/4KhM7GWM/WsrYKSWQekM7/4aeJETs4iH08ykDns7zWMAAsmefDFuJyW0VpYHCwlhscL8Io5ygWBgcWbUIcfsryfSfuS8CrjYjmfdn7AKRyCysk9fMvLtyxhMT2TQQAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_img(img, img_size=(64,64)):\n",
    "    '''\n",
    "    We preprocess the image by\n",
    "        - resize to shape of img_size: default 64x64\n",
    "        - convert to black and white (mode=\"1\")\n",
    "        - convert to numpy array\n",
    "        - map from 0,1 to -1,1\n",
    "        - flatten into 1d array as features\n",
    "    '''\n",
    "    res = img.resize(img_size)\n",
    "    res = res.convert(mode='1')\n",
    "    res = np.array(res)\n",
    "    res = np.where(res==0, -1, 1)\n",
    "    res = res.reshape((-1, 1))\n",
    "\n",
    "    return res\n",
    "\n",
    "def state2img(state: NDArray, img_size):\n",
    "    '''\n",
    "    To convert the retrieved state of classical hopfield network back to image, we do\n",
    "        - reshape from 1d array to img_size\n",
    "        - map from -1,1 to 0,1\n",
    "        - convert to Image type\n",
    "    '''\n",
    "    # check the post here: https://stackoverflow.com/questions/47290668/image-fromarray-just-produces-black-image\n",
    "    res = state.reshape(img_size)\n",
    "    res = np.where(res==-1, 0, 1)\n",
    "    res = (res*255).astype(np.uint8)\n",
    "    res = Image.fromarray(res, mode='L')\n",
    "    return res\n",
    "\n",
    "\n",
    "# let's read in an image\n",
    "IMG_FOLDER = \"./images/\"\n",
    "img1 = Image.open(\"./images/homer_bw.png\")\n",
    "# and see it's shape, mode, format\n",
    "print(img1.size, img1.mode, img1.format)\n",
    "# and take a look at it\n",
    "# img1.show()\n",
    "\n",
    "# now, preprocess it\n",
    "img_size = 64, 64\n",
    "# img_size = 4, 4\n",
    "train_img1 = preprocess_img(img1, img_size)\n",
    "print(train_img1.shape)\n",
    "\n",
    "# convert back and check it's the same\n",
    "state2img(train_img1, img_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to store the image and retrieve it with half of the image masked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAACfklEQVR4AZWWAZakMAhEdd/c/8ouBSaBonB2nTdtjNSnIJru+7l+O24LmKP+/Ca/oLcPP4nge2ZD95gOEe5CxsrJk4jz9nRfJdwk72o3edK10evf570IK4UpHw7u+0nRGNpMa+YHAIkLwS6oKEBTCBT76GZxKxcVobMDSe6TA4AXYBtrTRgApfqtVoMJ0L2+aragAb3ZJzkRNCAv35HK0QCQsXJSAr4qYIoC6GeIle+1Aowr4BqypwAUQqkJLwD/VcHVAa7/NpE9ybcR8tdpkIrtKvnJNB+H2t4mU61qsobMdUAkd/WycdnmXEykrK0HZx97kmiSX6KJh55V+IKQBztI1WoJF8MAsW1GYlhQ+xQBELWc5grS9AnwQAJk0c0JN3qlwJkAtobGkJEWnPELIp6DXUaNj3WwHlY8A3D7FdbAwUAvYetrKv+loIrrPVjF2Xn/LsG36mus1iWamLI8Vo//4fH2gliNJH3uDY1m4GKHlIvXqQDQKm65zcPIuXaEANSYlNVi0ctK4GVEC0qEXcTmglJAo6OtQip5he5fOqYnvFgFkcSYMWuk4g4ZRA9W4nJGHLL/gwNpAWrIm749yj0ijLh1+2glUBMnPYotJe0LAvQMO3IYMGDI86pbAW0Z0WZm8M5WrbCDIBjDQfE2irwH0h5lIzx798CoyunSEtX7i9ynY6avEpewCdSK05gz8tgBgFbUI5z2+QGAr4eMeMd2wj6XjwngT/4KhM7GWM/WsrYKSWQekM7/4aeJETs4iH08ykDns7zWMAAsmefDFuJyW0VpYHCwlhscL8Io5ygWBgcWbUIcfsryfSfuS8CrjYjmfdn7AKRyCysk9fMvLtyxhMT2TQQAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 64,64\n",
    "\n",
    "# preprocess the image\n",
    "processed_img1 = preprocess_img(img1, size) # (64*64, 1)\n",
    "\n",
    "# batch it\n",
    "store_patterns = np.expand_dims(processed_img1, axis=0)\n",
    "\n",
    "# get a masked image to test\n",
    "masked_img1 = processed_img1.copy()\n",
    "masked_img1[64*64//2:] = 0\n",
    "# print(masked_img1.shape)\n",
    "# state2img(masked_img1, (64,64)).show()\n",
    "\n",
    "# get our model\n",
    "hopfield = ClassicalHopfieldNetwork(64*64)\n",
    "\n",
    "# store the image\n",
    "hopfield.store(store_patterns)\n",
    "# print(hopfield.W)\n",
    "\n",
    "# retrieve the masked img\n",
    "retrieved_img1 = hopfield.retrieve(masked_img1)\n",
    "# print(retrieved_img1)\n",
    "\n",
    "# convert the retrieved numpy array back to Image\n",
    "res_img = state2img(retrieved_img1, (64,64))\n",
    "res_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if we store multiple patterns, how would that affect our retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color2bw(img, thr=150):\n",
    "    '''\n",
    "    Convert color image to black white image with PIL Image\n",
    "\n",
    "    Args:\n",
    "        img: a PIL Image\n",
    "        thr: a threshold for cutting between black and white pixel, default 150\n",
    "    \n",
    "    Return:\n",
    "        res: a PIL image of mode \"L\"\n",
    "    '''\n",
    "    res = img.convert('L').point(lambda x: 255 if x>thr else 0)\n",
    "    return res\n",
    "\n",
    "def get_bw_imgs(img_dir, pt_thr=150):\n",
    "    '''\n",
    "    Given img_dir, return all image files converted to L mode PIL black white images in that dir\n",
    "\n",
    "    Args:\n",
    "        img_dir: a dir path containing images for training\n",
    "        pt_thr: threshold for color2bw()\n",
    "    Return:\n",
    "        train_imgs: \"L\" mode black white PIL images\n",
    "    '''\n",
    "    train_imgs = os.listdir(img_dir)\n",
    "    train_imgs = [img_dir+f for f in train_imgs if os.path.isfile(img_dir+f)]\n",
    "    train_imgs = [Image.open(img) for img in train_imgs]\n",
    "    train_imgs = [color2bw(img, pt_thr) for img in train_imgs]\n",
    "    return train_imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model\n",
    "hopfield2 = ClassicalHopfieldNetwork(64*64)\n",
    "\n",
    "# get multiple store images\n",
    "TRAIN_IMG_PATH = IMG_FOLDER+'train/'\n",
    "train_imgs = get_bw_imgs(TRAIN_IMG_PATH)\n",
    "train_imgs[1].show()    # take a look at one of it\n",
    "\n",
    "# TBD: mask some train img and try retrieve\n",
    "# can do this by writing a new function doing the masking job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
